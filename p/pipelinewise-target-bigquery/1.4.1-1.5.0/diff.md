# Comparing `tmp/pipelinewise-target-bigquery-1.4.1.tar.gz` & `tmp/pipelinewise-target-bigquery-1.5.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "pipelinewise-target-bigquery-1.4.1.tar", last modified: Thu Nov 24 17:14:04 2022, max compression
+gzip compressed data, was "pipelinewise-target-bigquery-1.5.0.tar", last modified: Sat Jun 10 11:13:20 2023, max compression
```

## Comparing `pipelinewise-target-bigquery-1.4.1.tar` & `pipelinewise-target-bigquery-1.5.0.tar`

### file list

```diff
@@ -1,21 +1,21 @@
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-24 17:14:04.132131 pipelinewise-target-bigquery-1.4.1/
--rw-r--r--   0 runner    (1001) docker     (122)    10409 2022-11-24 17:13:56.000000 pipelinewise-target-bigquery-1.4.1/LICENSE
--rw-r--r--   0 runner    (1001) docker     (122)    22334 2022-11-24 17:14:04.132131 pipelinewise-target-bigquery-1.4.1/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (122)    21890 2022-11-24 17:13:56.000000 pipelinewise-target-bigquery-1.4.1/README.md
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-24 17:14:04.132131 pipelinewise-target-bigquery-1.4.1/pipelinewise_target_bigquery.egg-info/
--rw-r--r--   0 runner    (1001) docker     (122)    22334 2022-11-24 17:14:04.000000 pipelinewise-target-bigquery-1.4.1/pipelinewise_target_bigquery.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (122)      553 2022-11-24 17:14:04.000000 pipelinewise-target-bigquery-1.4.1/pipelinewise_target_bigquery.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (122)        1 2022-11-24 17:14:04.000000 pipelinewise-target-bigquery-1.4.1/pipelinewise_target_bigquery.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (122)       57 2022-11-24 17:14:04.000000 pipelinewise-target-bigquery-1.4.1/pipelinewise_target_bigquery.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (122)      150 2022-11-24 17:14:04.000000 pipelinewise-target-bigquery-1.4.1/pipelinewise_target_bigquery.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (122)       16 2022-11-24 17:14:04.000000 pipelinewise-target-bigquery-1.4.1/pipelinewise_target_bigquery.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (122)       38 2022-11-24 17:14:04.132131 pipelinewise-target-bigquery-1.4.1/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (122)     1185 2022-11-24 17:13:56.000000 pipelinewise-target-bigquery-1.4.1/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-24 17:14:04.132131 pipelinewise-target-bigquery-1.4.1/target_bigquery/
--rw-r--r--   0 runner    (1001) docker     (122)    17220 2022-11-24 17:13:56.000000 pipelinewise-target-bigquery-1.4.1/target_bigquery/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    28124 2022-11-24 17:13:56.000000 pipelinewise-target-bigquery-1.4.1/target_bigquery/db_sync.py
--rw-r--r--   0 runner    (1001) docker     (122)      783 2022-11-24 17:13:56.000000 pipelinewise-target-bigquery-1.4.1/target_bigquery/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (122)     2904 2022-11-24 17:13:56.000000 pipelinewise-target-bigquery-1.4.1/target_bigquery/flattening.py
--rw-r--r--   0 runner    (1001) docker     (122)     2581 2022-11-24 17:13:56.000000 pipelinewise-target-bigquery-1.4.1/target_bigquery/sql_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1444 2022-11-24 17:13:56.000000 pipelinewise-target-bigquery-1.4.1/target_bigquery/stream_ref_helper.py
--rw-r--r--   0 runner    (1001) docker     (122)     5292 2022-11-24 17:13:56.000000 pipelinewise-target-bigquery-1.4.1/target_bigquery/stream_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-10 11:13:20.544477 pipelinewise-target-bigquery-1.5.0/
+-rw-r--r--   0 runner    (1001) docker     (123)    10409 2023-06-10 11:13:10.000000 pipelinewise-target-bigquery-1.5.0/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)    24487 2023-06-10 11:13:20.544477 pipelinewise-target-bigquery-1.5.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    24043 2023-06-10 11:13:10.000000 pipelinewise-target-bigquery-1.5.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-10 11:13:20.544477 pipelinewise-target-bigquery-1.5.0/pipelinewise_target_bigquery.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)    24487 2023-06-10 11:13:20.000000 pipelinewise-target-bigquery-1.5.0/pipelinewise_target_bigquery.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)      553 2023-06-10 11:13:20.000000 pipelinewise-target-bigquery-1.5.0/pipelinewise_target_bigquery.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-10 11:13:20.000000 pipelinewise-target-bigquery-1.5.0/pipelinewise_target_bigquery.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       57 2023-06-10 11:13:20.000000 pipelinewise-target-bigquery-1.5.0/pipelinewise_target_bigquery.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      150 2023-06-10 11:13:20.000000 pipelinewise-target-bigquery-1.5.0/pipelinewise_target_bigquery.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       16 2023-06-10 11:13:20.000000 pipelinewise-target-bigquery-1.5.0/pipelinewise_target_bigquery.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       38 2023-06-10 11:13:20.544477 pipelinewise-target-bigquery-1.5.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     1185 2023-06-10 11:13:10.000000 pipelinewise-target-bigquery-1.5.0/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-10 11:13:20.544477 pipelinewise-target-bigquery-1.5.0/target_bigquery/
+-rw-r--r--   0 runner    (1001) docker     (123)    17389 2023-06-10 11:13:10.000000 pipelinewise-target-bigquery-1.5.0/target_bigquery/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28181 2023-06-10 11:13:10.000000 pipelinewise-target-bigquery-1.5.0/target_bigquery/db_sync.py
+-rw-r--r--   0 runner    (1001) docker     (123)      783 2023-06-10 11:13:10.000000 pipelinewise-target-bigquery-1.5.0/target_bigquery/exceptions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2904 2023-06-10 11:13:10.000000 pipelinewise-target-bigquery-1.5.0/target_bigquery/flattening.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2581 2023-06-10 11:13:10.000000 pipelinewise-target-bigquery-1.5.0/target_bigquery/sql_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1444 2023-06-10 11:13:10.000000 pipelinewise-target-bigquery-1.5.0/target_bigquery/stream_ref_helper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5292 2023-06-10 11:13:10.000000 pipelinewise-target-bigquery-1.5.0/target_bigquery/stream_utils.py
```

### Comparing `pipelinewise-target-bigquery-1.4.1/LICENSE` & `pipelinewise-target-bigquery-1.5.0/LICENSE`

 * *Files identical despite different names*

### Comparing `pipelinewise-target-bigquery-1.4.1/PKG-INFO` & `pipelinewise-target-bigquery-1.5.0/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pipelinewise-target-bigquery
-Version: 1.4.1
+Version: 1.5.0
 Summary: Singer.io target for loading data to BigQuery - PipelineWise compatible
 Home-page: https://github.com/transferwise/pipelinewise-target-bigquery
 Author: jmriego
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Programming Language :: Python :: 3 :: Only
 Description-Content-Type: text/markdown
 Provides-Extra: test
@@ -64,23 +64,24 @@
 | -------------------------------------   | --------- | ------------ | ---------------------------------------------------------------                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
 | project_id                              | String    | Yes          | BigQuery project                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
 | location                                | String    |              | Region where BigQuery stores your dataset                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
 | default_target_schema                   | String    |              | Name of the schema where the tables will be created. If `schema_mapping` is not defined then every stream sent by the tap is loaded into this schema.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
 | default_target_schema_select_permission | String    |              | Grant USAGE privilege on newly created schemas and grant SELECT privilege on newly created                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
 | schema_mapping                          | Object    |              | Useful if you want to load multiple streams from one tap to multiple BigQuery schemas.<br><br>If the tap sends the `stream_id` in `<schema_name>-<table_name>` format then this option overwrites the `default_target_schema` value. Note, that using `schema_mapping` you can overwrite the `default_target_schema_select_permission` value to grant SELECT permissions to different groups per schemas or optionally you can create indices automatically for the replicated tables.<br><br> **Note**: This is an experimental feature and recommended to use via PipelineWise YAML files that will generate the object mapping in the right JSON format. For further info check a [PipelineWise YAML Example](https://transferwise.github.io/pipelinewise/connectors/taps/mysql.html#configuring-what-to-replicate). |
 | batch_size_rows                         | Integer   |              | (Default: 100000) Maximum number of rows in each batch. At the end of each batch, the rows in the batch are loaded into BigQuery.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
-| batch_wait_limit_seconds                | Integer   |              | (Default: None) Maximum time to wait for batch to reach `batch_size_rows`. |
-| flush_all_streams                       | Boolean   |              | (Default: False) Flush and load every stream into BigQuery when one batch is full. Warning: This may trigger transfer of data with low number of records, and may cause performance problems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
+| batch_wait_limit_seconds                | Integer   |              | (Default: None) Maximum time to wait for batch to reach `batch_size_rows`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
+| flush_all_streams                       | Boolean   |              | (Default: False) Flush and load every stream into BigQuery when one batch is full. Warning: This may trigger transfer of data with low number of records, and may cause performance problems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
 | parallelism                             | Integer   |              | (Default: 0) The number of threads used to flush tables. 0 will create a thread for each stream, up to parallelism_max. -1 will create a thread for each CPU core. Any other positive number will create that number of threads, up to parallelism_max.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
 | max_parallelism                         | Integer   |              | (Default: 16) Max number of parallel threads to use when flushing tables.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
 | add_metadata_columns                    | Boolean   |              | (Default: False) Metadata columns add extra row level information about data ingestions, (i.e. when was the row read in source, when was inserted or deleted in bigquery etc.) Metadata columns are creating automatically by adding extra columns to the tables with a column prefix `_sdc_`. The column names are following the stitch naming conventions documented at https://www.stitchdata.com/docs/data-structure/integration-schemas#sdc-columns. Enabling metadata columns will flag the deleted rows by setting the `_sdc_deleted_at` metadata column. Without the `add_metadata_columns` option the deleted rows from singer taps will not be recognisable in BigQuery.                                                                                                                                      |
-| hard_delete                             | Boolean   |              | (Default: False) When `hard_delete` option is true then DELETE SQL commands will be performed in BigQuery to delete rows in tables. It's achieved by continuously checking the  `_sdc_deleted_at` metadata column sent by the singer tap. Due to deleting rows requires metadata columns, `hard_delete` option automatically enables the `add_metadata_columns` option as well.|
-| hard_delete_mapping                     | Object    |              | This is useful if you want to set `hard_delete` for some streams but not others. This should contain a mapping of `stream_id: <Boolean>`. This boolean will override the default behaviour set with `hard_delete` for that stream. If a stream is not defined in `hard_delete_mapping` it will behave according to `hard_delete`. When `hard_delete` option is true then DELETE SQL commands will be performed in BigQuery to delete rows in tables. It's achieved by continuously checking the  `_sdc_deleted_at` metadata column sent by the singer tap. Due to deleting rows requires metadata columns, `hard_delete` option automatically enables the `add_metadata_columns` option as well.|
+| hard_delete                             | Boolean   |              | (Default: False) When `hard_delete` option is true then DELETE SQL commands will be performed in BigQuery to delete rows in tables. It's achieved by continuously checking the  `_sdc_deleted_at` metadata column sent by the singer tap. Due to deleting rows requires metadata columns, `hard_delete` option automatically enables the `add_metadata_columns` option as well.                                                                                                                                                                                                                                                                                                                                                                                                                                         |
+| hard_delete_mapping                     | Object    |              | This is useful if you want to set `hard_delete` for some streams but not others. This should contain a mapping of `stream_id: <Boolean>`. This boolean will override the default behaviour set with `hard_delete` for that stream. If a stream is not defined in `hard_delete_mapping` it will behave according to `hard_delete`. When `hard_delete` option is true then DELETE SQL commands will be performed in BigQuery to delete rows in tables. It's achieved by continuously checking the  `_sdc_deleted_at` metadata column sent by the singer tap. Due to deleting rows requires metadata columns, `hard_delete` option automatically enables the `add_metadata_columns` option as well.                                                                                                                        |
 | data_flattening_max_level               | Integer   |              | (Default: 0) Object type RECORD items from taps can be loaded into VARIANT columns as JSON (default) or we can flatten the schema by creating columns automatically.<br><br>When value is 0 (default) then flattening functionality is turned off.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
 | primary_key_required                    | Boolean   |              | (Default: True) Log based and Incremental replications on tables with no Primary Key cause duplicates when merging UPDATE events. When set to true, stop loading data if no Primary Key is defined.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
+| append_only                             | Boolean   |              | (Default: False) Always append records and ignore any primary keys to update rows. This would cause duplicates when rows are updated but would save in ingestion costs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
 | validate_records                        | Boolean   |              | (Default: False) Validate every single record message to the corresponding JSON schema. This option is disabled by default and invalid RECORD messages will fail only at load time by BigQuery. Enabling this option will detect invalid records earlier but could cause performance degradation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
 | temp_schema                             | String    |              | Name of the schema where the temporary tables will be created. Will default to the same schema as the target tables                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
 | use_partition_pruning                   | Boolean   |              | (Default: False) If `true` then BigQuery table partition pruning will be used for tables which have partitioning enabled. This partitioning should be on a column which is immutable such as an integer primary key or a `created_at` column. The partitioning should be set up manually by the user. This feature can dramatically reduce the cost of each `MERGE` for large tables.                                                                                                                                                                                                                                                                                                                                                                                                                                   |
 
 
 ### Schema Changes
```

### Comparing `pipelinewise-target-bigquery-1.4.1/README.md` & `pipelinewise-target-bigquery-1.5.0/README.md`

 * *Files 2% similar despite different names*

```diff
@@ -52,23 +52,24 @@
 | -------------------------------------   | --------- | ------------ | ---------------------------------------------------------------                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
 | project_id                              | String    | Yes          | BigQuery project                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
 | location                                | String    |              | Region where BigQuery stores your dataset                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
 | default_target_schema                   | String    |              | Name of the schema where the tables will be created. If `schema_mapping` is not defined then every stream sent by the tap is loaded into this schema.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
 | default_target_schema_select_permission | String    |              | Grant USAGE privilege on newly created schemas and grant SELECT privilege on newly created                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
 | schema_mapping                          | Object    |              | Useful if you want to load multiple streams from one tap to multiple BigQuery schemas.<br><br>If the tap sends the `stream_id` in `<schema_name>-<table_name>` format then this option overwrites the `default_target_schema` value. Note, that using `schema_mapping` you can overwrite the `default_target_schema_select_permission` value to grant SELECT permissions to different groups per schemas or optionally you can create indices automatically for the replicated tables.<br><br> **Note**: This is an experimental feature and recommended to use via PipelineWise YAML files that will generate the object mapping in the right JSON format. For further info check a [PipelineWise YAML Example](https://transferwise.github.io/pipelinewise/connectors/taps/mysql.html#configuring-what-to-replicate). |
 | batch_size_rows                         | Integer   |              | (Default: 100000) Maximum number of rows in each batch. At the end of each batch, the rows in the batch are loaded into BigQuery.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
-| batch_wait_limit_seconds                | Integer   |              | (Default: None) Maximum time to wait for batch to reach `batch_size_rows`. |
-| flush_all_streams                       | Boolean   |              | (Default: False) Flush and load every stream into BigQuery when one batch is full. Warning: This may trigger transfer of data with low number of records, and may cause performance problems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
+| batch_wait_limit_seconds                | Integer   |              | (Default: None) Maximum time to wait for batch to reach `batch_size_rows`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
+| flush_all_streams                       | Boolean   |              | (Default: False) Flush and load every stream into BigQuery when one batch is full. Warning: This may trigger transfer of data with low number of records, and may cause performance problems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
 | parallelism                             | Integer   |              | (Default: 0) The number of threads used to flush tables. 0 will create a thread for each stream, up to parallelism_max. -1 will create a thread for each CPU core. Any other positive number will create that number of threads, up to parallelism_max.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
 | max_parallelism                         | Integer   |              | (Default: 16) Max number of parallel threads to use when flushing tables.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
 | add_metadata_columns                    | Boolean   |              | (Default: False) Metadata columns add extra row level information about data ingestions, (i.e. when was the row read in source, when was inserted or deleted in bigquery etc.) Metadata columns are creating automatically by adding extra columns to the tables with a column prefix `_sdc_`. The column names are following the stitch naming conventions documented at https://www.stitchdata.com/docs/data-structure/integration-schemas#sdc-columns. Enabling metadata columns will flag the deleted rows by setting the `_sdc_deleted_at` metadata column. Without the `add_metadata_columns` option the deleted rows from singer taps will not be recognisable in BigQuery.                                                                                                                                      |
-| hard_delete                             | Boolean   |              | (Default: False) When `hard_delete` option is true then DELETE SQL commands will be performed in BigQuery to delete rows in tables. It's achieved by continuously checking the  `_sdc_deleted_at` metadata column sent by the singer tap. Due to deleting rows requires metadata columns, `hard_delete` option automatically enables the `add_metadata_columns` option as well.|
-| hard_delete_mapping                     | Object    |              | This is useful if you want to set `hard_delete` for some streams but not others. This should contain a mapping of `stream_id: <Boolean>`. This boolean will override the default behaviour set with `hard_delete` for that stream. If a stream is not defined in `hard_delete_mapping` it will behave according to `hard_delete`. When `hard_delete` option is true then DELETE SQL commands will be performed in BigQuery to delete rows in tables. It's achieved by continuously checking the  `_sdc_deleted_at` metadata column sent by the singer tap. Due to deleting rows requires metadata columns, `hard_delete` option automatically enables the `add_metadata_columns` option as well.|
+| hard_delete                             | Boolean   |              | (Default: False) When `hard_delete` option is true then DELETE SQL commands will be performed in BigQuery to delete rows in tables. It's achieved by continuously checking the  `_sdc_deleted_at` metadata column sent by the singer tap. Due to deleting rows requires metadata columns, `hard_delete` option automatically enables the `add_metadata_columns` option as well.                                                                                                                                                                                                                                                                                                                                                                                                                                         |
+| hard_delete_mapping                     | Object    |              | This is useful if you want to set `hard_delete` for some streams but not others. This should contain a mapping of `stream_id: <Boolean>`. This boolean will override the default behaviour set with `hard_delete` for that stream. If a stream is not defined in `hard_delete_mapping` it will behave according to `hard_delete`. When `hard_delete` option is true then DELETE SQL commands will be performed in BigQuery to delete rows in tables. It's achieved by continuously checking the  `_sdc_deleted_at` metadata column sent by the singer tap. Due to deleting rows requires metadata columns, `hard_delete` option automatically enables the `add_metadata_columns` option as well.                                                                                                                        |
 | data_flattening_max_level               | Integer   |              | (Default: 0) Object type RECORD items from taps can be loaded into VARIANT columns as JSON (default) or we can flatten the schema by creating columns automatically.<br><br>When value is 0 (default) then flattening functionality is turned off.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
 | primary_key_required                    | Boolean   |              | (Default: True) Log based and Incremental replications on tables with no Primary Key cause duplicates when merging UPDATE events. When set to true, stop loading data if no Primary Key is defined.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
+| append_only                             | Boolean   |              | (Default: False) Always append records and ignore any primary keys to update rows. This would cause duplicates when rows are updated but would save in ingestion costs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
 | validate_records                        | Boolean   |              | (Default: False) Validate every single record message to the corresponding JSON schema. This option is disabled by default and invalid RECORD messages will fail only at load time by BigQuery. Enabling this option will detect invalid records earlier but could cause performance degradation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
 | temp_schema                             | String    |              | Name of the schema where the temporary tables will be created. Will default to the same schema as the target tables                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
 | use_partition_pruning                   | Boolean   |              | (Default: False) If `true` then BigQuery table partition pruning will be used for tables which have partitioning enabled. This partitioning should be on a column which is immutable such as an integer primary key or a `created_at` column. The partitioning should be set up manually by the user. This feature can dramatically reduce the cost of each `MERGE` for large tables.                                                                                                                                                                                                                                                                                                                                                                                                                                   |
 
 
 ### Schema Changes
```

### Comparing `pipelinewise-target-bigquery-1.4.1/pipelinewise_target_bigquery.egg-info/PKG-INFO` & `pipelinewise-target-bigquery-1.5.0/pipelinewise_target_bigquery.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pipelinewise-target-bigquery
-Version: 1.4.1
+Version: 1.5.0
 Summary: Singer.io target for loading data to BigQuery - PipelineWise compatible
 Home-page: https://github.com/transferwise/pipelinewise-target-bigquery
 Author: jmriego
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Programming Language :: Python :: 3 :: Only
 Description-Content-Type: text/markdown
 Provides-Extra: test
@@ -64,23 +64,24 @@
 | -------------------------------------   | --------- | ------------ | ---------------------------------------------------------------                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
 | project_id                              | String    | Yes          | BigQuery project                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
 | location                                | String    |              | Region where BigQuery stores your dataset                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
 | default_target_schema                   | String    |              | Name of the schema where the tables will be created. If `schema_mapping` is not defined then every stream sent by the tap is loaded into this schema.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
 | default_target_schema_select_permission | String    |              | Grant USAGE privilege on newly created schemas and grant SELECT privilege on newly created                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
 | schema_mapping                          | Object    |              | Useful if you want to load multiple streams from one tap to multiple BigQuery schemas.<br><br>If the tap sends the `stream_id` in `<schema_name>-<table_name>` format then this option overwrites the `default_target_schema` value. Note, that using `schema_mapping` you can overwrite the `default_target_schema_select_permission` value to grant SELECT permissions to different groups per schemas or optionally you can create indices automatically for the replicated tables.<br><br> **Note**: This is an experimental feature and recommended to use via PipelineWise YAML files that will generate the object mapping in the right JSON format. For further info check a [PipelineWise YAML Example](https://transferwise.github.io/pipelinewise/connectors/taps/mysql.html#configuring-what-to-replicate). |
 | batch_size_rows                         | Integer   |              | (Default: 100000) Maximum number of rows in each batch. At the end of each batch, the rows in the batch are loaded into BigQuery.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
-| batch_wait_limit_seconds                | Integer   |              | (Default: None) Maximum time to wait for batch to reach `batch_size_rows`. |
-| flush_all_streams                       | Boolean   |              | (Default: False) Flush and load every stream into BigQuery when one batch is full. Warning: This may trigger transfer of data with low number of records, and may cause performance problems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
+| batch_wait_limit_seconds                | Integer   |              | (Default: None) Maximum time to wait for batch to reach `batch_size_rows`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
+| flush_all_streams                       | Boolean   |              | (Default: False) Flush and load every stream into BigQuery when one batch is full. Warning: This may trigger transfer of data with low number of records, and may cause performance problems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
 | parallelism                             | Integer   |              | (Default: 0) The number of threads used to flush tables. 0 will create a thread for each stream, up to parallelism_max. -1 will create a thread for each CPU core. Any other positive number will create that number of threads, up to parallelism_max.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
 | max_parallelism                         | Integer   |              | (Default: 16) Max number of parallel threads to use when flushing tables.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
 | add_metadata_columns                    | Boolean   |              | (Default: False) Metadata columns add extra row level information about data ingestions, (i.e. when was the row read in source, when was inserted or deleted in bigquery etc.) Metadata columns are creating automatically by adding extra columns to the tables with a column prefix `_sdc_`. The column names are following the stitch naming conventions documented at https://www.stitchdata.com/docs/data-structure/integration-schemas#sdc-columns. Enabling metadata columns will flag the deleted rows by setting the `_sdc_deleted_at` metadata column. Without the `add_metadata_columns` option the deleted rows from singer taps will not be recognisable in BigQuery.                                                                                                                                      |
-| hard_delete                             | Boolean   |              | (Default: False) When `hard_delete` option is true then DELETE SQL commands will be performed in BigQuery to delete rows in tables. It's achieved by continuously checking the  `_sdc_deleted_at` metadata column sent by the singer tap. Due to deleting rows requires metadata columns, `hard_delete` option automatically enables the `add_metadata_columns` option as well.|
-| hard_delete_mapping                     | Object    |              | This is useful if you want to set `hard_delete` for some streams but not others. This should contain a mapping of `stream_id: <Boolean>`. This boolean will override the default behaviour set with `hard_delete` for that stream. If a stream is not defined in `hard_delete_mapping` it will behave according to `hard_delete`. When `hard_delete` option is true then DELETE SQL commands will be performed in BigQuery to delete rows in tables. It's achieved by continuously checking the  `_sdc_deleted_at` metadata column sent by the singer tap. Due to deleting rows requires metadata columns, `hard_delete` option automatically enables the `add_metadata_columns` option as well.|
+| hard_delete                             | Boolean   |              | (Default: False) When `hard_delete` option is true then DELETE SQL commands will be performed in BigQuery to delete rows in tables. It's achieved by continuously checking the  `_sdc_deleted_at` metadata column sent by the singer tap. Due to deleting rows requires metadata columns, `hard_delete` option automatically enables the `add_metadata_columns` option as well.                                                                                                                                                                                                                                                                                                                                                                                                                                         |
+| hard_delete_mapping                     | Object    |              | This is useful if you want to set `hard_delete` for some streams but not others. This should contain a mapping of `stream_id: <Boolean>`. This boolean will override the default behaviour set with `hard_delete` for that stream. If a stream is not defined in `hard_delete_mapping` it will behave according to `hard_delete`. When `hard_delete` option is true then DELETE SQL commands will be performed in BigQuery to delete rows in tables. It's achieved by continuously checking the  `_sdc_deleted_at` metadata column sent by the singer tap. Due to deleting rows requires metadata columns, `hard_delete` option automatically enables the `add_metadata_columns` option as well.                                                                                                                        |
 | data_flattening_max_level               | Integer   |              | (Default: 0) Object type RECORD items from taps can be loaded into VARIANT columns as JSON (default) or we can flatten the schema by creating columns automatically.<br><br>When value is 0 (default) then flattening functionality is turned off.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
 | primary_key_required                    | Boolean   |              | (Default: True) Log based and Incremental replications on tables with no Primary Key cause duplicates when merging UPDATE events. When set to true, stop loading data if no Primary Key is defined.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
+| append_only                             | Boolean   |              | (Default: False) Always append records and ignore any primary keys to update rows. This would cause duplicates when rows are updated but would save in ingestion costs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
 | validate_records                        | Boolean   |              | (Default: False) Validate every single record message to the corresponding JSON schema. This option is disabled by default and invalid RECORD messages will fail only at load time by BigQuery. Enabling this option will detect invalid records earlier but could cause performance degradation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
 | temp_schema                             | String    |              | Name of the schema where the temporary tables will be created. Will default to the same schema as the target tables                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
 | use_partition_pruning                   | Boolean   |              | (Default: False) If `true` then BigQuery table partition pruning will be used for tables which have partitioning enabled. This partitioning should be on a column which is immutable such as an integer primary key or a `created_at` column. The partitioning should be set up manually by the user. This feature can dramatically reduce the cost of each `MERGE` for large tables.                                                                                                                                                                                                                                                                                                                                                                                                                                   |
 
 
 ### Schema Changes
```

### Comparing `pipelinewise-target-bigquery-1.4.1/pipelinewise_target_bigquery.egg-info/SOURCES.txt` & `pipelinewise-target-bigquery-1.5.0/pipelinewise_target_bigquery.egg-info/SOURCES.txt`

 * *Files identical despite different names*

### Comparing `pipelinewise-target-bigquery-1.4.1/setup.py` & `pipelinewise-target-bigquery-1.5.0/setup.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 from setuptools import setup
 
 with open('README.md') as f:
     long_description = f.read()
 
 setup(name="pipelinewise-target-bigquery",
-      version="1.4.1",
+      version="1.5.0",
       description="Singer.io target for loading data to BigQuery - PipelineWise compatible",
       long_description=long_description,
       long_description_content_type='text/markdown',
       author="jmriego",
       url='https://github.com/transferwise/pipelinewise-target-bigquery',
       classifiers=[
           'License :: OSI Approved :: Apache Software License',
```

### Comparing `pipelinewise-target-bigquery-1.4.1/target_bigquery/__init__.py` & `pipelinewise-target-bigquery-1.5.0/target_bigquery/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -25,15 +25,15 @@
 LOGGER = get_logger('target_bigquery')
 logging.getLogger('bigquery.connector').setLevel(logging.WARNING)
 
 DEFAULT_BATCH_SIZE_ROWS = 100000
 DEFAULT_PARALLELISM = 0  # 0 The number of threads used to flush tables
 DEFAULT_MAX_PARALLELISM = 16  # Don't use more than this number of threads by default when flushing streams in parallel
 DEFAULT_HARD_DELETE = False
-
+DEFAULT_APPEND_ONLY = False
 
 
 def add_metadata_columns_to_schema(schema_message):
     """Metadata _sdc columns according to the stitch documentation at
     https://www.stitchdata.com/docs/data-structure/integration-schemas#sdc-columns
 
     Metadata columns gives information about data injections
@@ -204,15 +204,15 @@
             # cause duplicates when merging UPDATE events.
             # Stop loading data by default if no Primary Key.
             #
             # If you want to load tables with no Primary Key:
             #  1) Set ` 'primary_key_required': false ` in the target-bigquery config.json
             #  or
             #  2) Use fastsync [postgres-to-bigquery, mysql-to-bigquery, etc.]
-            if config.get('primary_key_required', True) and len(o['key_properties']) == 0:
+            if config.get('primary_key_required', True) and not config.get('append_only', False) and len(o['key_properties']) == 0:
                 LOGGER.critical("Primary key is set to mandatory but not defined in the [{}] stream".format(stream))
                 raise Exception("key_properties field is required")
 
             key_properties[stream] = o['key_properties']
 
             if config.get('add_metadata_columns') or hard_delete_mapping.get(stream, default_hard_delete):
                 stream_to_sync[stream] = DbSync(config, add_metadata_columns_to_schema(o))
@@ -289,14 +289,15 @@
     :param filter_streams: Keys of streams to flush from the streams dict. Default is every stream
     :return: State dict with flushed positions
     :return: Dictionary with flush timestamps for each stream flushed
     """
     parallelism = config.get("parallelism", DEFAULT_PARALLELISM)
     max_parallelism = config.get("max_parallelism", DEFAULT_MAX_PARALLELISM)
     default_hard_delete = config.get("hard_delete", DEFAULT_HARD_DELETE)
+    default_append_only = config.get("append_only", DEFAULT_APPEND_ONLY)
     hard_delete_mapping = config.get("hard_delete_mapping", {})
 
     # Parallelism 0 means auto parallelism:
     #
     # Auto parallelism trying to flush streams efficiently with auto defined number
     # of threads where the number of threads is the number of streams that need to
     # be loaded but it's not greater than the value of max_parallelism
@@ -338,15 +339,15 @@
         # If we only have one stream to sync let's not introduce overhead.
         # for stream in streams_to_flush:
         load_stream_batch(
             stream=streams_to_flush[0],
             records_to_load=streams[streams_to_flush[0]],
             row_count=row_count,
             db_sync=stream_to_sync[streams_to_flush[0]],
-            delete_rows=hard_delete_mapping.get(streams_to_flush[0], default_hard_delete)
+            delete_rows=hard_delete_mapping.get(streams_to_flush[0], default_hard_delete) and not default_append_only
         )
 
     # reset flushed stream records to empty to avoid flushing same records
     # reset row count for flushed streams
     for stream in streams_to_flush:
         streams[stream] = {}
         row_count[stream] = 0
```

### Comparing `pipelinewise-target-bigquery-1.4.1/target_bigquery/db_sync.py` & `pipelinewise-target-bigquery-1.5.0/target_bigquery/db_sync.py`

 * *Files 2% similar despite different names*

```diff
@@ -384,15 +384,15 @@
         job_config.use_avro_logical_types = True
         job_config.write_disposition = 'WRITE_TRUNCATE'
         job = self.client.load_table_from_file(f, temp_table_ref, job_config=job_config)
         job.result()
         temp_table = self.client.get_table(temp_table_ref)
 
         pk_columns_names = primary_column_names(self.stream_schema_message)
-        if pk_columns_names:
+        if pk_columns_names and not self.connection_config.get('append_only', False):
             # TODO: make temp table creation and DML atomic with merge
             query = sql_utils.merge_from_table_sql(temp_table,
                                                    target_table,
                                                    self.column_names(),
                                                    self.renamed_columns,
                                                    pk_columns_names)
         else:
```

### Comparing `pipelinewise-target-bigquery-1.4.1/target_bigquery/exceptions.py` & `pipelinewise-target-bigquery-1.5.0/target_bigquery/exceptions.py`

 * *Files identical despite different names*

### Comparing `pipelinewise-target-bigquery-1.4.1/target_bigquery/flattening.py` & `pipelinewise-target-bigquery-1.5.0/target_bigquery/flattening.py`

 * *Files identical despite different names*

### Comparing `pipelinewise-target-bigquery-1.4.1/target_bigquery/sql_utils.py` & `pipelinewise-target-bigquery-1.5.0/target_bigquery/sql_utils.py`

 * *Files identical despite different names*

### Comparing `pipelinewise-target-bigquery-1.4.1/target_bigquery/stream_ref_helper.py` & `pipelinewise-target-bigquery-1.5.0/target_bigquery/stream_ref_helper.py`

 * *Files identical despite different names*

### Comparing `pipelinewise-target-bigquery-1.4.1/target_bigquery/stream_utils.py` & `pipelinewise-target-bigquery-1.5.0/target_bigquery/stream_utils.py`

 * *Files identical despite different names*

